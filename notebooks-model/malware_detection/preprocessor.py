import os
import re
import math
import hashlib
import binascii
import numpy as np
import pandas as pd
from typing import Dict, List, Union, Optional, Any, BinaryIO
from collections import Counter
import logging

# Configure logging
logger = logging.getLogger("defensys.malware-detection")

class Preprocessor:
    """Preprocessor for malware detection model.
    
    Extracts features from binary files for malware detection, including header information,
    entropy calculations, byte distributions, and structural properties.
    """
    
    def __init__(self):
        self.feature_columns = [
            # File metadata features
            'file_size', 'file_type_is_pe', 'file_type_is_elf', 'file_type_is_script',
            
            # Entropy features
            'entropy_whole', 'entropy_first_kb', 'entropy_last_kb',
            
            # Byte distribution features
            'printable_chars_ratio', 'null_bytes_ratio', 'executable_bytes_ratio',
            'highest_byte_entropy_section', 'avg_byte_entropy',
            
            # PE-specific features (if applicable)
            'pe_has_debug', 'pe_has_signature', 'pe_has_tls', 'pe_has_resources',
            'pe_import_count', 'pe_export_count', 'pe_section_count',
            'pe_has_configuration', 'pe_has_nx', 'pe_has_dep',
            'pe_suspicious_sections', 'pe_section_entropy_avg',
            
            # Import/DLL features (if applicable)
            'pe_has_networking_imports', 'pe_has_crypto_imports',
            'pe_has_virtualization_imports', 'pe_has_process_manipulation_imports',
            'pe_has_privilege_imports', 'pe_dll_count', 'pe_unique_import_count',
            
            # String features
            'has_urls', 'url_count', 'has_ips', 'has_registry_keys',
            'has_suspicious_strings', 'suspicious_string_count'
        ]
    
    def preprocess(self, data):
        """Preprocess file data for model input.
        
        Args:
            data: Dictionary, DataFrame, bytes, or file path to process
            
        Returns:
            DataFrame with extracted features ready for model input
        """
        # Handle different input formats
        if isinstance(data, dict):
            # Dictionary with file data or paths
            if 'file_path' in data:
                return self._process_file_path(data['file_path'])
            elif 'file_bytes' in data:
                return self._process_bytes(data['file_bytes'])
            else:
                raise ValueError("Input dictionary must contain 'file_path' or 'file_bytes' key")
        elif isinstance(data, pd.DataFrame):
            # DataFrame with multiple files
            if 'file_path' in data.columns:
                features = pd.DataFrame()
                for idx, row in data.iterrows():
                    file_features = self._process_file_path(row['file_path'])
                    features = pd.concat([features, file_features], ignore_index=True)
                return features
            else:
                raise ValueError("Input DataFrame must contain 'file_path' column")
        elif isinstance(data, bytes) or isinstance(data, bytearray):
            # Raw bytes
            return self._process_bytes(data)
        elif isinstance(data, str) and os.path.exists(data):
            # File path
            return self._process_file_path(data)
        else:
            raise ValueError("Input must be a dictionary with 'file_path' or 'file_bytes' key, DataFrame with 'file_path' column, bytes, or file path")
    
    def _process_file_path(self, file_path: str) -> pd.DataFrame:
        """Process a file from its path.
        
        Args:
            file_path: Path to the file to process
            
        Returns:
            DataFrame with extracted features
        """
        try:
            # Read file bytes
            with open(file_path, 'rb') as f:
                file_bytes = f.read()
            
            # Extract features
            features_dict = self._extract_features(file_bytes, file_path=file_path)
            
            # Convert to DataFrame
            features_df = pd.DataFrame([features_dict])
            
            # Ensure all feature columns exist
            for col in self.feature_columns:
                if col not in features_df.columns:
                    features_df[col] = 0
            
            # Return only needed columns in correct order
            return features_df[self.feature_columns]
        
        except Exception as e:
            # On error, create a DataFrame with default values
            default_features = {col: 0 for col in self.feature_columns}
            default_features['error'] = str(e)
            return pd.DataFrame([default_features])[self.feature_columns]
    
    def _process_bytes(self, file_bytes: bytes) -> pd.DataFrame:
        """Process raw file bytes.
        
        Args:
            file_bytes: Raw file bytes to process
            
        Returns:
            DataFrame with extracted features
        """
        try:
            # Extract features
            features_dict = self._extract_features(file_bytes)
            
            # Convert to DataFrame
            features_df = pd.DataFrame([features_dict])
            
            # Ensure all feature columns exist
            for col in self.feature_columns:
                if col not in features_df.columns:
                    features_df[col] = 0
            
            # Return only needed columns in correct order
            return features_df[self.feature_columns]
        
        except Exception as e:
            # On error, create a DataFrame with default values
            default_features = {col: 0 for col in self.feature_columns}
            default_features['error'] = str(e)
            return pd.DataFrame([default_features])[self.feature_columns]
    
    def _extract_features(self, file_bytes: bytes, file_path: Optional[str] = None) -> Dict[str, Any]:
        """Extract features from file bytes.
        
        Args:
            file_bytes: Raw file bytes
            file_path: Original file path if available
            
        Returns:
            Dictionary of extracted features
        """
        features = {}
        
        # Basic file metadata
        features['file_size'] = len(file_bytes)
        
        # Detect file type
        try:
            # Try to use magic library if available
            try:
                import magic
                file_type = magic.from_buffer(file_bytes[:4096], mime=True)
                features['file_type_is_pe'] = 1 if 'application/x-dosexec' in file_type else 0
                features['file_type_is_elf'] = 1 if 'application/x-elf' in file_type or 'application/x-sharedlib' in file_type else 0
                features['file_type_is_script'] = 1 if 'text/' in file_type else 0
            except ImportError:
                # Fallback to checking file signatures
                logger.warning("Magic library not available, using file signatures for type detection")
                # Check PE signature
                if file_bytes.startswith(b'MZ'):
                    features['file_type_is_pe'] = 1
                    features['file_type_is_elf'] = 0
                    features['file_type_is_script'] = 0
                # Check ELF signature
                elif file_bytes.startswith(b'\x7fELF'):
                    features['file_type_is_pe'] = 0
                    features['file_type_is_elf'] = 1
                    features['file_type_is_script'] = 0
                # Simple check for scripts - high ratio of ASCII characters
                else:
                    ascii_count = sum(1 for b in file_bytes[:4096] if 32 <= b <= 126)
                    if len(file_bytes[:4096]) > 0 and ascii_count / len(file_bytes[:4096]) > 0.7:
                        features['file_type_is_pe'] = 0
                        features['file_type_is_elf'] = 0
                        features['file_type_is_script'] = 1
                    else:
                        features['file_type_is_pe'] = 0
                        features['file_type_is_elf'] = 0
                        features['file_type_is_script'] = 0
        except Exception as e:
            logger.warning(f"Error detecting file type: {str(e)}")
            # Default values if everything fails
            features['file_type_is_pe'] = 0
            features['file_type_is_elf'] = 0
            features['file_type_is_script'] = 0
        
        # Calculate entropy of entire file
        features['entropy_whole'] = self._calculate_entropy(file_bytes)
        
        # Calculate entropy of first and last KB
        features['entropy_first_kb'] = self._calculate_entropy(file_bytes[:1024]) if len(file_bytes) >= 1024 else features['entropy_whole']
        features['entropy_last_kb'] = self._calculate_entropy(file_bytes[-1024:]) if len(file_bytes) >= 1024 else features['entropy_whole']
        
        # Calculate byte distributions
        printable_count = sum(1 for b in file_bytes if 32 <= b <= 126)
        null_count = sum(1 for b in file_bytes if b == 0)
        executable_count = sum(1 for b in file_bytes if b in (0x55, 0x8B, 0xEC, 0x83, 0xEC, 0x53, 0x56, 0x57))
        
        features['printable_chars_ratio'] = printable_count / len(file_bytes) if len(file_bytes) > 0 else 0
        features['null_bytes_ratio'] = null_count / len(file_bytes) if len(file_bytes) > 0 else 0
        features['executable_bytes_ratio'] = executable_count / len(file_bytes) if len(file_bytes) > 0 else 0
        
        # Default values for PE-specific features
        features['pe_has_debug'] = 0
        features['pe_has_signature'] = 0
        features['pe_has_tls'] = 0
        features['pe_has_resources'] = 0
        features['pe_import_count'] = 0
        features['pe_export_count'] = 0
        features['pe_section_count'] = 0
        features['pe_has_configuration'] = 0
        features['pe_has_nx'] = 0
        features['pe_has_dep'] = 0
        features['pe_suspicious_sections'] = 0
        features['pe_section_entropy_avg'] = 0
        features['pe_has_networking_imports'] = 0
        features['pe_has_crypto_imports'] = 0
        features['pe_has_virtualization_imports'] = 0
        features['pe_has_process_manipulation_imports'] = 0
        features['pe_has_privilege_imports'] = 0
        features['pe_dll_count'] = 0
        features['pe_unique_import_count'] = 0
        
        # Process PE files if applicable
        if features['file_type_is_pe']:
            try:
                pe_features = self._process_pe_file(file_bytes)
                features.update(pe_features)
            except Exception as e:
                # If PE parsing fails, keep default values
                pass
        
        # Process strings
        string_features = self._extract_string_features(file_bytes)
        features.update(string_features)
        
        # Section entropy (may be set by PE processing or calculated here)
        if 'highest_byte_entropy_section' not in features:
            chunk_size = 1024
            if len(file_bytes) >= chunk_size:
                entropies = [self._calculate_entropy(file_bytes[i:i+chunk_size]) 
                           for i in range(0, len(file_bytes), chunk_size) if i+chunk_size <= len(file_bytes)]
                features['highest_byte_entropy_section'] = max(entropies) if entropies else 0
                features['avg_byte_entropy'] = sum(entropies) / len(entropies) if entropies else 0
            else:
                features['highest_byte_entropy_section'] = features['entropy_whole']
                features['avg_byte_entropy'] = features['entropy_whole']
        
        return features
    
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data.
        
        Args:
            data: Bytes to calculate entropy for
            
        Returns:
            Entropy value between 0 and 8
        """
        if not data:
            return 0
        
        # Count byte occurrences
        byte_count = Counter(data)
        file_size = len(data)
        
        # Calculate entropy
        entropy = 0
        for count in byte_count.values():
            probability = count / file_size
            entropy -= probability * math.log2(probability)
        
        return entropy
    
    def _process_pe_file(self, file_bytes: bytes) -> Dict[str, Any]:
        """Extract features from a PE file.
        
        Args:
            file_bytes: Raw PE file bytes
            
        Returns:
            Dictionary of PE-specific features
        """
        features = {}
        
        try:
            # Try to import pefile if available
            try:
                import pefile
                pe_available = True
            except ImportError:
                logger.warning("pefile library not available, using simplified PE analysis")
                pe_available = False
                # Return basic features without detailed PE analysis
                return self._simplified_pe_analysis(file_bytes)
            
            # Continue with pefile analysis
            pe = pefile.PE(data=file_bytes)
            
            # Debug information
            features['pe_has_debug'] = 1 if hasattr(pe, 'DIRECTORY_ENTRY_DEBUG') else 0
            
            # Digital signature
            features['pe_has_signature'] = 1 if hasattr(pe, 'DIRECTORY_ENTRY_SECURITY') else 0
            
            # TLS (Thread Local Storage)
            features['pe_has_tls'] = 1 if hasattr(pe, 'DIRECTORY_ENTRY_TLS') else 0
            
            # Resources
            features['pe_has_resources'] = 1 if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0
            
            # Import information
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                imports = []
                dlls = []
                
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_name = entry.dll.decode('utf-8', 'ignore').lower() if entry.dll else ''
                    dlls.append(dll_name)
                    
                    if entry.imports:
                        for imp in entry.imports:
                            if imp.name:
                                imp_name = imp.name.decode('utf-8', 'ignore').lower()
                                imports.append(imp_name)
                
                features['pe_import_count'] = len(imports)
                features['pe_dll_count'] = len(dlls)
                features['pe_unique_import_count'] = len(set(imports))
                
                # Network-related imports
                network_dlls = ['ws2_32.dll', 'wsock32.dll', 'wininet.dll', 'winsock.dll']
                network_imports = ['socket', 'connect', 'recv', 'send', 'url', 'http', 'ftp']
                features['pe_has_networking_imports'] = 1 if any(dll in network_dlls for dll in dlls) or \
                                                      any(any(imp_str in imp for imp_str in network_imports) \
                                                          for imp in imports) else 0
                
                # Crypto-related imports
                crypto_imports = ['crypt', 'encrypt', 'decrypt', 'aes', 'des', 'rc4', 'md5', 'sha']
                features['pe_has_crypto_imports'] = 1 if any(any(imp_str in imp for imp_str in crypto_imports) \
                                                         for imp in imports) else 0
                
                # Virtualization-related imports
                virt_imports = ['virtual', 'vm', 'vmware', 'xen', 'qemu', 'sandbox']
                features['pe_has_virtualization_imports'] = 1 if any(any(imp_str in imp for imp_str in virt_imports) \
                                                              for imp in imports) else 0
                
                # Process manipulation imports
                process_imports = ['process', 'inject', 'thread', 'memory', 'debug', 'create']
                features['pe_has_process_manipulation_imports'] = 1 if any(any(imp_str in imp for imp_str in process_imports) \
                                                                      for imp in imports) else 0
                
                # Privilege-related imports
                priv_imports = ['admin', 'privilege', 'security', 'token']
                features['pe_has_privilege_imports'] = 1 if any(any(imp_str in imp for imp_str in priv_imports) \
                                                           for imp in imports) else 0
            
            # Export information
            if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') and pe.DIRECTORY_ENTRY_EXPORT and \
               pe.DIRECTORY_ENTRY_EXPORT.symbols:
                features['pe_export_count'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
            else:
                features['pe_export_count'] = 0
            
            # Configuration directory
            features['pe_has_configuration'] = 1 if hasattr(pe, 'DIRECTORY_ENTRY_LOAD_CONFIG') else 0
            
            # PE header characteristics
            features['pe_has_nx'] = 1 if pe.OPTIONAL_HEADER.DllCharacteristics & 0x0100 else 0  # NX compat flag
            features['pe_has_dep'] = 1 if pe.OPTIONAL_HEADER.DllCharacteristics & 0x0100 else 0  # DEP compat flag
            
            # Section analysis
            if hasattr(pe, 'sections'):
                features['pe_section_count'] = len(pe.sections)
                
                suspicious_section_names = [b'.text', b'UPX', b'packer', b'encrypt']
                suspicious_count = 0
                section_entropies = []
                
                for section in pe.sections:
                    section_name = section.Name.rstrip(b'\x00').decode('utf-8', 'ignore')
                    section_data = section.get_data()
                    
                    if section_data:
                        section_entropy = self._calculate_entropy(section_data)
                        section_entropies.append(section_entropy)
                    
                    # Check for suspicious section names
                    if any(susp in section.Name for susp in suspicious_section_names):
                        suspicious_count += 1
                
                features['pe_suspicious_sections'] = suspicious_count
                features['pe_section_entropy_avg'] = sum(section_entropies) / len(section_entropies) if section_entropies else 0
                
                # Find highest entropy section
                if section_entropies:
                    features['highest_byte_entropy_section'] = max(section_entropies)
            
            # Close the PE file
            pe.close()
            
        except Exception as e:
            # If PE file can't be parsed, set default values
            pass
        
        return features
    
    def _extract_string_features(self, file_bytes: bytes) -> Dict[str, Any]:
        """Extract string-related features from file bytes.
        
        Args:
            file_bytes: Raw file bytes
            
        Returns:
            Dictionary of string-related features
        """
        features = {}
        
        # Convert bytes to string for regex
        try:
            file_str = file_bytes.decode('utf-8', 'ignore')
        except:
            file_str = str(file_bytes)
        
        # Find URLs
        url_pattern = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[\w/._-]*')
        urls = url_pattern.findall(file_str)
        features['has_urls'] = 1 if urls else 0
        features['url_count'] = len(urls)
        
        # Find IP addresses
        ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        ips = ip_pattern.findall(file_str)
        features['has_ips'] = 1 if ips else 0
        
        # Find registry keys
        reg_pattern = re.compile(r'HKEY_|HKLM|HKCU|HKCR')
        reg_keys = reg_pattern.findall(file_str)
        features['has_registry_keys'] = 1 if reg_keys else 0
        
        # Detect suspicious strings
        suspicious_patterns = [
            r'inject', r'shellcode', r'exploit', r'payload',
            r'vulnerability', r'trojan', r'backdoor', r'rootkit',
            r'keylog', r'spy', r'steal', r'ransom', r'crypt',
            r'tor\b', r'onion\b', r'darknet', r'underground',
            r'black[- ]?hat', r'hack', r'crack', r'leak',
            r'dump', r'sniff', r'intercept', r'mitm'
        ]
        
        suspicious_count = 0
        for pattern in suspicious_patterns:
            matches = re.findall(pattern, file_str, re.IGNORECASE)
            suspicious_count += len(matches)
        
        features['has_suspicious_strings'] = 1 if suspicious_count > 0 else 0
        features['suspicious_string_count'] = suspicious_count
        
        return features
    
    def _simplified_pe_analysis(self, file_bytes: bytes) -> Dict[str, Any]:
        """Perform a simplified analysis of PE files when pefile is not available.
        
        Args:
            file_bytes: Raw PE file bytes
            
        Returns:
            Dictionary of basic PE features
        """
        features = {}
        
        # Initialize with default values
        features['pe_has_debug'] = 0
        features['pe_has_signature'] = 0
        features['pe_has_tls'] = 0
        features['pe_has_resources'] = 0
        features['pe_import_count'] = 0
        features['pe_export_count'] = 0
        features['pe_section_count'] = 0
        features['pe_has_configuration'] = 0
        features['pe_has_nx'] = 0
        features['pe_has_dep'] = 0
        features['pe_suspicious_sections'] = 0
        features['pe_section_entropy_avg'] = 0
        features['pe_has_networking_imports'] = 0
        features['pe_has_crypto_imports'] = 0
        features['pe_has_virtualization_imports'] = 0
        features['pe_has_process_manipulation_imports'] = 0
        features['pe_has_privilege_imports'] = 0
        features['pe_dll_count'] = 0
        features['pe_unique_import_count'] = 0
        
        try:
            # Look for PE sections with simple heuristics
            # DOS header is always at the beginning
            if not file_bytes.startswith(b'MZ'):
                return features
            
            # Find PE header offset (e_lfanew at offset 0x3C)
            if len(file_bytes) > 0x40:
                pe_offset = int.from_bytes(file_bytes[0x3C:0x40], byteorder='little')
                
                # Check if PE header is valid
                if len(file_bytes) > pe_offset + 4 and file_bytes[pe_offset:pe_offset+4] == b'PE\0\0':
                    # Simple section count - rough estimate
                    section_count = 0
                    for i in range(0, min(10, len(file_bytes) // 512)):
                        section_start = pe_offset + 0xF8 + i * 0x28
                        if len(file_bytes) > section_start + 8:
                            section_name = file_bytes[section_start:section_start+8]
                            if all(32 <= b <= 126 or b == 0 for b in section_name):
                                section_count += 1
                    
                    features['pe_section_count'] = section_count
                    
                    # Find some common strings to estimate import count
                    text = file_bytes.decode('ascii', errors='ignore')
                    
                    # Network-related strings
                    network_imports = ['socket', 'connect', 'inet', 'http', 'url', 'download', 'ftp']
                    if any(imp in text.lower() for imp in network_imports):
                        features['pe_has_networking_imports'] = 1
                    
                    # Crypto-related strings
                    crypto_imports = ['crypt', 'aes', 'rsa', 'sha', 'md5', 'hash']
                    if any(imp in text.lower() for imp in crypto_imports):
                        features['pe_has_crypto_imports'] = 1
                    
                    # Process manipulation strings
                    process_imports = ['process', 'inject', 'memory', 'virtual', 'alloc']
                    if any(imp in text.lower() for imp in process_imports):
                        features['pe_has_process_manipulation_imports'] = 1
                    
                    # Estimate DLL count
                    dll_pattern = r'\.dll'
                    import re
                    dll_matches = re.findall(dll_pattern, text, re.IGNORECASE)
                    features['pe_dll_count'] = len(set(dll_matches))
                    
                    # Estimate import count based on common API names
                    common_apis = ['Get', 'Create', 'Open', 'Close', 'Read', 'Write', 'Alloc', 'Free']
                    api_count = sum(text.count(api) for api in common_apis)
                    features['pe_import_count'] = min(api_count, 1000)  # Cap to reasonable value
        except Exception as e:
            logger.warning(f"Error in simplified PE analysis: {str(e)}")
        
        return features
        
    def feature_engineering(self, data):
        """Apply feature engineering to extracted features.
        
        This method is used for any additional transformations or normalizations
        after basic feature extraction.
        
        Args:
            data: DataFrame with basic extracted features
            
        Returns:
            DataFrame with engineered features
        """
        if not isinstance(data, pd.DataFrame):
            raise ValueError("Input must be a DataFrame")
        
        # Make a copy to avoid modifying the original
        df = data.copy()
        
        # Normalize numeric features
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        
        # Ensure file_size is capped and log-transformed
        if 'file_size' in df.columns:
            # Cap the file size to avoid outliers
            df['file_size'] = np.log1p(df['file_size'].clip(upper=1e9))
        
        # Create composite features
        if 'entropy_whole' in df.columns and 'file_type_is_pe' in df.columns:
            # High entropy in PE files is more suspicious
            df['entropy_pe_factor'] = df['entropy_whole'] * df['file_type_is_pe']
        
        if 'url_count' in df.columns and 'pe_has_networking_imports' in df.columns:
            # Combining URL presence with networking imports
            df['network_url_factor'] = df['url_count'] * df['pe_has_networking_imports']
        
        # Ensure all required columns are present
        for col in self.feature_columns:
            if col not in df.columns:
                df[col] = 0
        
        # Return only the required columns in the correct order
        return df[self.feature_columns]
